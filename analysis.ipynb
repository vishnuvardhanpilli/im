{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f09996",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Product_IDs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4848\\357544071.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m bills = pd.read_csv(\u001b[33m\"bills.csv\"\u001b[39m)\n\u001b[32m     15\u001b[39m products = pd.read_csv(\u001b[33m\"products.csv\"\u001b[39m)\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Merge product details into bills\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m bills = bills.merge(products, on=\u001b[33m\"Product_IDs\"\u001b[39m,how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Convert Date column to datetime\u001b[39;00m\n\u001b[32m     21\u001b[39m bills[\u001b[33m\"Date_Time\"\u001b[39m] = pd.to_datetime(bills[\u001b[33m\"Date_Time\"\u001b[39m], errors=\u001b[33m\"coerce\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\Vishnu Vardhan\\.vscode\\aac\\aac\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\Vishnu Vardhan\\.vscode\\aac\\aac\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\Vishnu Vardhan\\.vscode\\aac\\aac\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\Vishnu Vardhan\\.vscode\\aac\\aac\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1296\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1297\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1299\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1300\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1301\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\Vishnu Vardhan\\.vscode\\aac\\aac\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Product_IDs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import ast\n",
    "\n",
    "# ----------------------\n",
    "# Data Loading & Cleaning\n",
    "# ----------------------\n",
    "bills = pd.read_csv(\"bills.csv\")\n",
    "products = pd.read_csv(\"products.csv\")\n",
    "\n",
    "# Merge product details into bills\n",
    "bills = bills.merge(products, on=\"Product_IDs\",how=\"left\")\n",
    "\n",
    "# Convert Date column to datetime   \n",
    "bills[\"Date_Time\"] = pd.to_datetime(bills[\"Date_Time\"], errors=\"coerce\")\n",
    "\n",
    "# Clean Quantity column (handle strings like \"[8, 10, 8]\")\n",
    "def clean_quantity(val):\n",
    "    if isinstance(val, str) and val.startswith(\"[\"):\n",
    "        try:\n",
    "            return sum(ast.literal_eval(val))\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "bills[\"Quantity\"] = bills[\"Quantity\"].apply(clean_quantity)\n",
    "\n",
    "# ----------------------\n",
    "# Build complete weekly time series\n",
    "# ----------------------\n",
    "all_weeks = pd.date_range(bills[\"Date_Time\"].min(), bills[\"Date_Time\"].max(), freq=\"W-MON\")\n",
    "\n",
    "weekly_full = []\n",
    "for pid in bills[\"Product_ID\"].unique():\n",
    "    product_weekly = (\n",
    "        bills[bills[\"Product_ID\"] == pid]\n",
    "        .groupby(pd.Grouper(key=\"Date_Time\", freq=\"W-MON\"))[\"Quantity\"]\n",
    "        .sum()\n",
    "        .reindex(all_weeks, fill_value=0)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"Date_Time\"})\n",
    "    )\n",
    "    product_weekly[\"Product_ID\"] = pid\n",
    "    weekly_full.append(product_weekly)\n",
    "\n",
    "weekly = pd.concat(weekly_full, ignore_index=True)\n",
    "\n",
    "# Monthly aggregation\n",
    "monthly = bills.groupby([\"Product_ID\", pd.Grouper(key=\"Date_Time\", freq=\"M\")])[\"Quantity\"].sum().reset_index()\n",
    "\n",
    "# ----------------------\n",
    "# Scaling per product\n",
    "# ----------------------\n",
    "scalers = {}\n",
    "weekly[\"Quantity_scaled\"] = 0.0\n",
    "\n",
    "for pid in weekly[\"Product_ID\"].unique():\n",
    "    mask = weekly[\"Product_ID\"] == pid\n",
    "    scaler = MinMaxScaler()\n",
    "    values = weekly.loc[mask, [\"Quantity\"]].values.astype(float)\n",
    "    if values.max() > values.min():  # normal scaling\n",
    "        weekly.loc[mask, \"Quantity_scaled\"] = scaler.fit_transform(values)\n",
    "    else:  # constant series\n",
    "        weekly.loc[mask, \"Quantity_scaled\"] = 0.0\n",
    "        scaler.fit(np.array([[0.0], [1.0]]))\n",
    "    scalers[pid] = scaler\n",
    "\n",
    "# ----------------------\n",
    "# Dataset for PyTorch\n",
    "# ----------------------\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, df, product_id, seq_len=6):\n",
    "        product_df = df[df[\"Product_ID\"] == product_id].copy()\n",
    "        product_df = product_df.set_index(\"Date_Time\").asfreq(\"W-MON\", fill_value=0)\n",
    "        self.data = product_df[\"Quantity_scaled\"].values.astype(float)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.data) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx + self.seq_len >= len(self.data):\n",
    "            idx = len(self.data) - self.seq_len - 1\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+self.seq_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ----------------------\n",
    "# Model Definition (GRU + ReLU)\n",
    "# ----------------------\n",
    "class SalesGRU(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=1, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "# ----------------------\n",
    "# Train Model\n",
    "# ----------------------\n",
    "def train_model(product_id, epochs=50, seq_len=6):\n",
    "    dataset = SalesDataset(weekly, product_id, seq_len)\n",
    "    if len(dataset) < 2:  # too little data\n",
    "        return None, 0, 0, 0\n",
    "\n",
    "    split = max(1, int(len(dataset) * 0.8))\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [split, len(dataset) - split])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = SalesGRU()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X = X.unsqueeze(-1)\n",
    "            y = y.unsqueeze(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.unsqueeze(-1)\n",
    "            preds = model(X).squeeze()\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_true.extend(y.numpy())\n",
    "\n",
    "    scaler = scalers[product_id]\n",
    "    all_preds = scaler.inverse_transform(np.array(all_preds).reshape(-1,1)).flatten()\n",
    "    all_true = scaler.inverse_transform(np.array(all_true).reshape(-1,1)).flatten()\n",
    "\n",
    "    if len(all_true) > 0:\n",
    "        mae = mean_absolute_error(all_true, all_preds)\n",
    "        rmse = mean_squared_error(all_true, all_preds, squared=False)\n",
    "        mape = np.mean(np.abs((all_true - all_preds) / np.maximum(all_true, 1e-5))) * 100\n",
    "        acc = max(0, 100 - mape)\n",
    "    else:\n",
    "        mae, rmse, acc = 0, 0, 0\n",
    "\n",
    "    return model, mae, rmse, acc\n",
    "\n",
    "# ----------------------\n",
    "# Predict Future\n",
    "# ----------------------\n",
    "def predict_future(model, product_id, seq_len=6, weeks=8):\n",
    "    dataset = SalesDataset(weekly, product_id, seq_len)\n",
    "    seq = dataset.data[-seq_len:]\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(weeks):\n",
    "        x = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x).item()\n",
    "        preds.append(pred)\n",
    "        seq = np.append(seq[1:], pred)\n",
    "\n",
    "    scaler_local = scalers[product_id]\n",
    "    preds_inverse = scaler_local.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "\n",
    "    future_dates = pd.date_range(start=weekly[\"Date_Time\"].max() + pd.Timedelta(weeks=1), periods=weeks, freq=\"W-MON\")\n",
    "    return pd.DataFrame({\"Week\": future_dates, \"Predicted\": preds_inverse})\n",
    "\n",
    "# ----------------------\n",
    "# Extra Analysis\n",
    "# ----------------------\n",
    "def average_sales():\n",
    "    weekly_avg = weekly.groupby(\"Product_ID\")[\"Quantity\"].mean().reset_index(name=\"Avg_Weekly_Sales\")\n",
    "    monthly_avg = monthly.groupby(\"Product_ID\")[\"Quantity\"].mean().reset_index(name=\"Avg_Monthly_Sales\")\n",
    "    return weekly_avg, monthly_avg\n",
    "\n",
    "def brand_distribution():\n",
    "    return bills.groupby([\"Brand\", \"Product_ID\"])[\"Quantity\"].sum().reset_index(name=\"Total_Sales\")\n",
    "\n",
    "# ----------------------\n",
    "# Run for all products & Save Excel\n",
    "# ----------------------\n",
    "all_future_preds = {}\n",
    "accuracy_records = []\n",
    "\n",
    "for pid in weekly[\"Product_ID\"].unique():\n",
    "    print(f\"\\nTraining model for {pid}...\")\n",
    "    model, mae, rmse, acc = train_model(pid, epochs=50, seq_len=6)\n",
    "\n",
    "    if model is not None:\n",
    "        future_preds = predict_future(model, pid, weeks=12)\n",
    "        all_future_preds[pid] = future_preds\n",
    "\n",
    "    accuracy_records.append({\"Product_ID\": pid, \"MAE\": mae, \"RMSE\": rmse, \"Accuracy(%)\": acc})\n",
    "\n",
    "weekly_avg, monthly_avg = average_sales()\n",
    "brand_sales = brand_distribution()\n",
    "accuracy_df = pd.DataFrame(accuracy_records)\n",
    "\n",
    "with pd.ExcelWriter(\"sales_outputs.xlsx\") as writer:\n",
    "    for pid, preds in all_future_preds.items():\n",
    "        preds.to_excel(writer, sheet_name=f\"Future_{pid[:10]}\", index=False)\n",
    "    weekly_avg.to_excel(writer, sheet_name=\"Weekly_Avg\", index=False)\n",
    "    monthly_avg.to_excel(writer, sheet_name=\"Monthly_Avg\", index=False)\n",
    "    brand_sales.to_excel(writer, sheet_name=\"Brand_Sales\", index=False)\n",
    "    accuracy_df.to_excel(writer, sheet_name=\"Accuracy\", index=False)\n",
    "\n",
    "print(\"\\nAll outputs saved to sales_outputs.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aac (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
